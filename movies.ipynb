{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar Keras, TensorFlow y otras librerías útiles\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] ....\n",
      "\n",
      "TRAIN LABELS: [1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Importar el dataset\n",
    "\n",
    "imdb_dataset = keras.datasets.imdb\n",
    "\n",
    "\"\"\"\n",
    "Este dataset es utilizado para la clasificación binaria de reseñas (positivas o negativas) de peliculas.\n",
    "Consiste en 25.000 reseñas tomadas de IMDB, etiquetadas segun sentimiento (positivo o negativo).\n",
    "Estas reseñas ya han sido pre procesadas, y cada una codificada como una secuencia de índices (enteros).\n",
    "Estos indices represan palabras que aparecen en la reseña.\n",
    "Estas palabras fueron indexadas según su frecuencia en el dataset, es decir el índice 5 representa a la\n",
    "quinta palabra más frecuente en el dataset. Esto permite un rápido filtro durante opeaciones tales como\n",
    "considerar solo las 5000 palabras más frecuentes del vocabularios y otros.\n",
    "\"\"\"\n",
    "\n",
    "# Al cargar el dataset se obtienen 4 arreglos NumPy\n",
    "\"\"\"\n",
    "x_train, x_test: list of sequences, which are lists of indexes (integers).\n",
    "    If the num_words argument was specific, the maximum possible index value is num_words-1.\n",
    "    If the maxlen argument was specified, the largest possible sequence length is maxlen.\n",
    "    \n",
    "y_train, y_test: list of integer labels (1 for positive or 0 for negative).\n",
    "\"\"\"\n",
    "\n",
    "maximum_index = 20000\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb_dataset.load_data(num_words=maximum_index)\n",
    "\n",
    "\n",
    "print(\"TRAIN DATA:\", train_data[0], \"....\\n\")\n",
    "print(\"TRAIN LABELS:\", train_labels)\n",
    "\n",
    "# Si quisieramos cambiar la proporción de training data vs test data podemos concatenar los datasets y volver\n",
    "# a dividirlos\n",
    "\n",
    "all_data = np.concatenate((train_data, test_data), axis=0)\n",
    "all_labels = np.concatenate((train_labels, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  # this film requires a lot of patience because it focuses on mood and character development the plot is very simple and many of the scenes take place on the same set in frances austen's the sandy dennis character apartment but the film builds to a disturbing climax br br the characters create an atmosphere rife with sexual tension and psychological trickery it's very interesting that robert altman directed this considering the style and structure of his other films still the trademark altman audio style is evident here and there i think what really makes this film work is the brilliant performance by sandy dennis it's definitely one of her darker characters but she plays it so perfectly and convincingly that it's scary michael burns does a good job as the mute young man regular altman player michael murphy has a small part the # moody set fits the content of the story very well in short this movie is a powerful study of loneliness sexual repression and desperation be patient # up the atmosphere and pay attention to the wonderfully written script br br i praise robert altman this is one of his many films that deals with unconventional fascinating subject matter this film is disturbing but it's sincere and it's sure to elicit a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video stores you may have to buy it off the internet \n",
      "\n",
      "Label:  1\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de como se ve una de las reviews\n",
    "indice_ejemplo = 0\n",
    "\n",
    "index = imdb_dataset.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in test_data[indice_ejemplo]] )\n",
    "print(\"Review: \", decoded, \"\\n\")\n",
    "print(\"Label: \", test_labels[indice_ejemplo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización del training y test set.\n",
    "\n",
    "def normalizar(secuencias, dimension = maximum_index):\n",
    "    #relleno con 0 cuando la dimension es menor\n",
    "    normalizado = np.zeros((len(secuencias), dimension))\n",
    "    for i, secuencia in enumerate(secuencias):\n",
    "        normalizado[i, secuencia] = 1\n",
    "    return normalizado\n",
    "\n",
    "normalized_data = normalizar(all_data)\n",
    "# Convierto los labels a floats\n",
    "float_labels = np.array(all_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separo el dataset en los casos utilizados para entrenamiento y para pruebas.\n",
    "factor_entrenamiento = 80 #porcentaje para entrenamiento, el resto para testing\n",
    "cantidad_casos = len(normalized_data)\n",
    "cantidad_entrenamiento = (int)((cantidad_casos * factor_entrenamiento) / 100) \n",
    "\n",
    "test_data = normalized_data[cantidad_entrenamiento:]\n",
    "test_labels = float_labels[cantidad_entrenamiento:]\n",
    "train_data = normalized_data[:cantidad_entrenamiento]\n",
    "train_labels = float_labels[:cantidad_entrenamiento]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                1000050   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,005,201\n",
      "Trainable params: 1,005,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "# Input - Layer\n",
    "model.add(keras.layers.Dense(50, activation = \"relu\", input_shape=(maximum_index, )))\n",
    "# Hidden - Layers\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "# Dropout selecciona neuronas al azar para ignorar durante el entrenamiento, previene overfitting\n",
    "model.add(keras.layers.Dense(50, activation = \"relu\"))\n",
    "model.add(keras.layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(keras.layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila el modelo, utiliza Adam como optimizador\n",
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "40000/40000 [==============================] - 10s 241us/sample - loss: 0.0472 - accuracy: 0.9843 - val_loss: 0.4501 - val_accuracy: 0.8954\n",
      "Epoch 2/4\n",
      "40000/40000 [==============================] - 11s 265us/sample - loss: 0.0334 - accuracy: 0.9887 - val_loss: 0.5019 - val_accuracy: 0.8909\n",
      "Epoch 3/4\n",
      "40000/40000 [==============================] - 11s 263us/sample - loss: 0.0269 - accuracy: 0.9908 - val_loss: 0.5657 - val_accuracy: 0.8880\n",
      "Epoch 4/4\n",
      "40000/40000 [==============================] - 7s 176us/sample - loss: 0.0258 - accuracy: 0.9913 - val_loss: 0.5506 - val_accuracy: 0.8927\n"
     ]
    }
   ],
   "source": [
    "results = model.fit(\n",
    " train_data, train_labels,\n",
    " epochs = 4,\n",
    " batch_size = 500,\n",
    " validation_data = (test_data, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.89175\n"
     ]
    }
   ],
   "source": [
    "print(\"Precisión del modelo:\" , np.mean(results.history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reseña:  # this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all \n",
      "\n",
      "Esperado :  1\n",
      "[[3.2303329e-03]\n",
      " [1.0000000e+00]\n",
      " [9.8074728e-01]\n",
      " ...\n",
      " [2.9196674e-03]\n",
      " [5.7552392e-05]\n",
      " [6.1206770e-01]]\n",
      "Prediccion: [0.00323033]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo tomo una review\n",
    "review_index = 0\n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in all_data[review_index]] )\n",
    "print(\"Reseña: \", decoded, \"\\n\")\n",
    "print(\"Esperado : \", all_labels[review_index])\n",
    "\n",
    "### Ejemplo de predicción con modelo entrenado ###\n",
    "\n",
    "prediccion = model.predict(test_data)\n",
    "\n",
    "print(prediccion)\n",
    "print(\"Prediccion:\" , prediccion[review_index])\n",
    "\n",
    "# Obtener el caracter con mayores posibilidades de matchear\n",
    "\n",
    "#print(\"Índice dentro del array del caracter con mayores probabilidades de matchear:\", np.argmax(predictions[0]))\n",
    "\n",
    "# Examinar el valor asociado a caracter\n",
    "\n",
    "#print(\"Dígito predecido:\",test_labels[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
